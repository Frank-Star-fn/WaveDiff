##### Table of contents
1. [Installation](#Installation)
2. [Dataset preparation](#Dataset-preparation)
3. [How to run](#How-to-run)
4. [Results](#Results)
5. [Evaluation](#Evaluation)
6. [Acknowledgments](#Acknowledgments)
7. [Contacts](#Contacts)

# Official PyTorch implementation of "Wavelet Diffusion Models are fast and scalable Image Generators"
<div align="center">
    <img width="1000" alt="teaser" src="assets/single_wavelet.png"/>
</div>

WaveDiff is a novel wavelet-based diffusion structure that employs low-and-high frequency components of wavelet subbands from both image and feature levels. These are adaptively implemented to accelerate the sampling process while maintaining good generation quality. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets show that WaveDiff provides state-of-the-art training and inference speed, which serves as a stepping-stone to offering real-time and high-fidelity diffusion models.


Details of the model architecture and experimental results can be found in [our following paper]():
```bibtex
@article{hao2022wavelet,
  title={Wavelet Diffusion Models are fast and scalable Image Generators},
  author={Hao Phung and Quan Dao and Anh Tran},
  journal={arXiv preprint arXiv:<submit_number>},
  year={2022}
}
```
 **Please CITE** our paper whenever this repository is used to help produce published results or incorporated into other software.

## Installation ##
Latest Pytorch version is required. 

Install neccessary libraries:
```bash
pip install -r requirements.txt
```
For `pytorch_wavelets`, please follow [here](https://github.com/fbcotter/pytorch_wavelets.git).

## Dataset preparation ##
We trained on four datasets, including CIFAR10, STL10, LSUN Church Outdoor 256 and CelebA HQ (256 & 512). 

For CIFAR10 and STL10, they will be automatically downloaded in the first time execution. 

For CelebA HQ (256) and LSUN, please check out [here](https://github.com/NVlabs/NVAE#set-up-file-paths-and-data) for dataset preparation.

For CelebA HQ (512), please download data at [here](https://drive.google.com/file/d/1E23HCNL-v9c54Wnzkm9yippBW8IaLUXp/view?usp=share_link) and then generate [LMDB](https://lmdb.readthedocs.io/en/release/) format dataset by [Torch Toolbox](https://github.com/PistonY/torch-toolbox#5-make-and-use-lmdb-dataset). 

Once a dataset is downloaded, please put it in `data/` directory as follows:
```
data/
├── STL-10
├── celeba
├── celeba_512
├── cifar-10
└── lsun
```

## How to run ##
We provide a bash script for our experiments on different datasets. The syntax is following:
```
bash run.sh <DATASET> <MODE> <#GPUS>
```
where: 
- `<DATASET>`: `cifar10`, `stl10`, `celeba_256`, `celeba_512`, and `lsun`.
- `<MODE>`: `train` and `test`.
- `<#GPUS>`: the number of gpus (e.g. 1, 2, 4, 8).

Note, please set agrument `--exp` correspondingly for both `train` and `test` mode. All of detailed configurations are well set in [run.sh](./run.sh). 

**GPU allocation**: Our work is experimented on NVIDIA 40GB A100 GPUs. Namely, we use a single GPU for CIFAR10 and STL10, 2 GPUs for CelebA-HQ 256, 4 GPUs for LSUN, and 8 GPUs for CelebA-HQ 512. 

## Results ##
Model performance and pretrained checkpoints are provided as below:
<table>
  <tr>
    <th>Model</th>
    <th>FID</th>
    <th>Recall</th>
    <th>Time (s)</th>
    <th>Checkpoints</th>
  </tr>
  <tr>
    <td>CIFAR-10</td>
    <td>4.01</td>
    <td>0.55</td>
    <td>0.08</td>
    <td><a href="https://www.dropbox.com/s/dkr8af9nmscane0/netG_1300.pth?dl=0">netG_1300.pth</a></td>
  </tr>
  <tr>
    <td>STL-10</td>
    <td>12.93</td>
    <td>0.41</td>
    <td>0.38</td>
    <td><a href="https://www.dropbox.com/s/whqois9i3fvpp5d/netG_600.pth?dl=0">netG_600.pth</a></td>
  </tr>
  <tr>
    <td>CelebA-HQ (256 x 256) </td>
    <td>5.94</td>
    <td>0.37</td>
    <td>0.79</td>
    <td><a href="https://www.dropbox.com/s/1am1e4e9c9h7cab/netG_475.pth?dl=0">netG_475.pth</a></td>
  </tr>
  <tr>
    <td>CelebA-HQ (512 x 512) </td>
    <td>6.40</td>
    <td>0.35</td>
    <td>0.59</td>
    <td><a href="https://www.dropbox.com/s/7ifkutwd51mmwgk/netG_350.pth?dl=0">netG_350.pth</a></td>
  </tr>
  <tr>
    <td>LSUN Church</td>
    <td>5.06</td>
    <td>0.40</td>
    <td>1.54</td>
    <td><a href="https://www.dropbox.com/s/395l9um0bre67r8/netG_400.pth?dl=0">netG_400.pth</a></td>
  </tr>
</table>

Inference time is computed over 300 trials on a single NVIDIA A100 GPU for a batch size of 100, except for the one of high-resolution CelebA-HQ $(512 \times 512)$ is computed for a batch of 25 samples.

Downloaded pretrained models should be put at `saved_info/wdd_gan/<DATASET>/` directory where all `<DATASET>` are listed in [How to run](#how-to-run) section.

## Evaluation ##
### Inference ###
Samples can be generated by calling [run.sh](./run.sh) with `test` mode.

### FID ###
To compute fid of pretrained models on a specific epoch, we can add additional arguments including ```--compute_fid``` and ```--real_img_dir /path/to/real/images``` of the corresponding experiments in [run.sh](./run.sh).

### Recall ###
We adopt the official Pytorch implementation of [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada-pytorch.git) to compute Recall of generated samples.

## Acknowledgments
Thanks to Xiao et al for releasing their official implementation of the [DDGAN](https://github.com/NVlabs/denoising-diffusion-gan.git) paper.

## Contacts ##
If you have any problems, please open an issue in this repository or ping an email to [tienhaophung@gmail.com](mailto:tienhaophung@gmail.com).
